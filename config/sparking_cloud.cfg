[General Settings]
enable_logging = Yes
cluster_names = [al-cluster-1]
cluster_instances_root_folder = cluster_instances/
key_root_folder = key/
cloud_provider_names = [AWS]
configuration_rules = Configuration_Rules_1

[Logging Settings]
log_to_file = Yes
log_to_console = Yes
file_name = logging/sparking_cloud.log
file_mode = a
encoding = utf-8
level = DEBUG
format = %%(asctime)s.%%(msecs)03d %%(levelname)s: %%(message)s
date_format = %%Y/%%m/%%d %%H:%%M:%%S

[AWS Settings]
config_file_path = ~/.aws/config
credentials_file_path = ~/.aws/credentials
service = ec2

[al-cluster-1 Settings]
master_instances_settings = [AWS_Master_Instances_Settings_1]
worker_instances_settings = [AWS_Worker_Instances_Settings_1]

[AWS_Master_Instances_Settings_1 Settings]
number_of_master_instances = 1
ami_id = ami-id
operating_system = Linux/UNIX
username = ubuntu
ssh_port = 22
type = t3.micro
key_name = key
security_group_ids = [sg-id]
prefix_name = master
market_type = spot
spot_max_price = Current_EC2_Spot_Instance_Price
spot_type = one-time
spot_interruption_behavior = terminate
placement = us-east-1c
subnet_id = subnet-id

[AWS_Worker_Instances_Settings_1 Settings]
number_of_worker_instances = 1
ami_id = ami-id
operating_system = Linux/UNIX
username = ubuntu
ssh_port = 22
type = t3.micro
key_name = key
security_group_ids = [sg-id]
prefix_name = worker
market_type = spot
spot_max_price = Current_EC2_Spot_Instance_Price
spot_type = one-time
spot_interruption_behavior = terminate
placement = us-east-1c
subnet_id = subnet-id

[Configuration_Rules_1 Settings]
max_tries = 10
time_between_retries_in_seconds = 2
verbose_scripts = False
store_remote_host_public_key_to_guest_known_hosts = True
store_remote_host_public_key_script_file = script/store_remote_host_public_key_on_known_hosts.sh
key_types = ecdsa
known_hosts_file = ~/.ssh/known_hosts
install_hadoop = Yes
hadoop_setup_on_master_script_file = script/hadoop_setup_on_master.sh
hadoop_setup_on_worker_script_file = script/hadoop_setup_on_worker.sh
hadoop_version = 3.3.1
install_spark = Yes
spark_setup_on_master_script_file = script/spark_setup_on_master.sh
spark_setup_on_worker_script_file = script/spark_setup_on_worker.sh
spark_version = 3.2.0
master_port = 7077
master_webui_port = 8080
worker_cores = maximum
worker_memory = maximum
worker_memory_unit = KB
worker_port = 7078
worker_webui_port = 8081
properties_file = config/spark_defaults.conf
pool_properties_file = config/spark_scheduler.xml
application_folder = application/app_folder/
application_entry_point = application/app_folder/main.py
application_arguments = argument1 argument2 argumentN
send_local_input_folder = Yes
input_folder = input/app_folder/

